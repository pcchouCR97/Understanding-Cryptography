{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Support Vector Machine in PennyLane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src = \"../../images_QML/maksym-kaharlytskyi-wine-unsplash.jpg\" alt=\"maksym-kaharlytskyi-wine-unsplash\" \n",
    "    style=\"width: 450px; height: 300px;\">\n",
    "    <p style=\"font-size: 16px; font-style: italic; color: gray; margin-top: 5px;\">\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Photo by <a href=\"https://unsplash.com/@qwitka?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Maksym Kaharlytskyi</a> on <a href=\"https://unsplash.com/photos/four-wine-glasses-3uJt73tr4hI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will give a introduction of how to use Quantum Support Vector Mechine in PennyLane. For demonstrate purpose, we will work on 2 categories of `load_wine`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Workflow \n",
    "1.  Preparing data set from load_wine\n",
    "2.  PennyLane and scikit-learn\n",
    "3.  Dataset dimension reduction \n",
    "4.  Custom feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set from load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import Sklean package [`load_wine`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html) to explore our data. The wine recofnition dataset a labeled dataset with information about wines. There are 13 numerical variables that describe the color intensitym alcohol concentration, and other things. \n",
    "\n",
    "*   Learn more about tutorial for `load_wine`: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html\n",
    "\n",
    "*   Find out raw data of the `load_wine`: https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import `load_wine` dataset\n",
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(seed=5678)\n",
    "\n",
    "x,y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `load_wine`, the first 59 elements must belong to the first category (label 0) while the 71 sybsequent belongs to the second one (label 1). We can run the following code to obtain a labeled dataset with two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      " x data\n",
      " [[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.179e+01 2.130e+00 2.780e+00 ... 9.700e-01 2.440e+00 4.660e+02]\n",
      " [1.237e+01 1.630e+00 2.300e+00 ... 8.900e-01 2.780e+00 3.420e+02]\n",
      " [1.204e+01 4.300e+00 2.380e+00 ... 7.900e-01 2.570e+00 5.800e+02]]\n",
      "==========\n",
      " The first row from the raw data: \n",
      " [1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " Number of x variables : 13\n",
      "==========\n",
      " y data\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's print out our dataset\n",
    "x = x[:59+71]\n",
    "y = y[:59+71]\n",
    "\n",
    "# print out x and y to explore our data\n",
    "print(\"=\"*10)\n",
    "print(f\" x data\\n {x}\")\n",
    "print(\"=\"*10)\n",
    "print(f\" The first row from the raw data: \\n {x[0]}\")\n",
    "print(f\" Number of x variables : {len(x[0])}\")\n",
    "print(\"=\"*10)\n",
    "print(f\" y data\\n {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "\n",
    "*   `x` is the data set includes the 13 variables and `y` is the corresponding of the `x`.\n",
    "*   `0` represents the category 0 and `1` represents the category 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call `train_test_split` from `sklearn.model_selection` to help us split our training and test dataset. We set the `training_size = 0.9` as we take 90% of the raw data from x and y as a training set and the remaining 10% as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets with training size of 0.9.\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x,y, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any quantum operations, we should normalized our dataset. We will use the easiest way of normalization ways: scaling each of the variables linearly in such way that the maximum absolute value taken by each variable be 1. \n",
    "\n",
    "Please run `MaxAbsScaler()` object from `sklearn.preprocessing` to normalize our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Normalize our training data into a range of {0,1}\n",
    "scaler = MaxAbsScaler()\n",
    "x_tr = scaler.fit_transform(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of our data are positive number between 0 and 1. It is worth to notice that we don't normalize the test ***simultaneously*** since we will includes the information from the training set. Instead, we normalize the test set independently from the training set. Please run the following code to normalize the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our test set independently from the training set to avoid includes training set data leaks into test set.\n",
    "x_test = scaler.transform(x_test)\n",
    "x_test = np.clip(x_test,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PennyLane and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since out dataset has 13 variables, using angle encoding or the ZZ feature map need 13 qubtis. This maight not be feasible if we want our kernal to be simulated on some powerful computer. Therefore, we will work on amplitude encoding using 4 qubots, which can encode up to 16 qubits, and we will refill the remaining qubits with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the installation instruction for `lightning.qubit`:\n",
    "\n",
    "https://docs.pennylane.ai/projects/lightning/en/stable/lightning_qubit/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "\n",
    "# initialize our quantum device\n",
    "nqubits = 4\n",
    "dev = qml.device(name = 'default.qubit', wires = nqubits)\n",
    "\n",
    "# define our kernel circuit\n",
    "@qml.qnode(dev)\n",
    "def kernel_circ(a, b):\n",
    "    # encode a into amplitude vector of n qubits\n",
    "    qml.AmplitudeEmbedding(\n",
    "        a, wires=range(nqubits), pad_with=0, normalize=True\n",
    "    )\n",
    "    # Conpute the inverse(adjoint) of the amplitude encoding of b\n",
    "    qml.adjoint(qml.AmplitudeEmbedding(\n",
    "        b, wires=range(nqubits), pad_with=0, normalize=True\n",
    "    ))\n",
    "    return qml.probs(wires=range(nqubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "1.  We use `AmplitudeEmbedding()` to returns an operation equivalent to the amplitude encoding of its first arguemnt. In our case, we used `a` and `b` for this first argument. And `AmplitudeEmbedding` encodes $2^n$ features into the amplitude vector of $n$ qubits.\n",
    "2.  There are the classical data that our kernel function takes as input. More, we also asked `AmplitudeEmbedding()` to noramlize each input vector for us, just as amplitude encoding needs us to do (as we were told in basic quantum state).\n",
    "3.  We use `pad_with = 0` to fill the remaining qubits with zeros since we are only consider `nqubits = 4` case out of total of 13 cases.\n",
    "4.  We implemented `qml.adjoint()` to compute the adjoint (inverse) of the amplitude encoding `b`.\n",
    "5.  Lastly, we retrive the probabilities of measuring each possible state in the computational basis by using `qml.probs(wires=range(nqubits))`. **The first element of this array will be the output of our kernel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that if the amplitude encoding feature map is given an inputs $x_{0},\\cdots,x_{2^{n}-1}$, it simply prepares the state \n",
    "\n",
    "$$\n",
    "| \\phi(\\overrightarrow{a}\\rangle) = \\frac{1}{\\sqrt{\\sum_{k}x_{k}^{2}}} \\sum_{k=0}^{2^{n}-1}x_{k}|k\\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the below to check if the circuit works as expected. The first entry should return 1, which corresponds to the output of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.00000000e+00, 9.90011286e-35, 1.49909334e-32, 2.57335126e-33,\n",
       "        1.47137684e-32, 8.95091217e-34, 4.65150803e-33, 9.97555849e-35,\n",
       "        4.93038066e-32, 1.26313737e-34, 7.40552773e-33, 4.06038555e-33,\n",
       "        1.37899948e-32, 6.66163615e-34, 1.50394221e-33, 2.17790719e-36], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_circ(x_tr[0],x_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the following code to train our model. In order to use a custom kernel, you are required to provide a `kernel` function accepting two arrays, `A` and `B`, and returning a matrix with entries `(j,k)` containing the kernel applied to `A[j]` and `B[k]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def qkernal(A, B):\n",
    "    # [0] for return the first entry: the measured value.\n",
    "    return np.array([[kernel_circ(a,b)[0] for b in B] for a in A] )\n",
    "\n",
    "# Fit the model\n",
    "svm = SVC(kernel=qkernal).fit(x_tr, y_tr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training can take up to a few mintues depends on the performance of your computer. Once it's over, you can check the accuracy of your trained model with the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amplitude encoding test score: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score_test_amp = accuracy_score(svm.predict(x_test), y_test)\n",
    "print(f\"Amplitude encoding test score: {score_test_amp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our casem this gives an accuracy of 0.92, meaning that the SVM is capable of classifying most of the elements in the test dataset correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to use amplitude encoding to take full advantage of the 13 variables of our dataset while only using 4 qubits. Now, let's see how can we reduce the number of variables in the dataset - while trying to minimize the loss of information - and thus be able to use other feature maps that could perhaps yield better results.\n",
    "\n",
    "Here, we will try to reduce the number of variables in our dataset into 8 and we train our QVSM with the new angle encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we are going to use in this section is called **principal component analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle directions\n",
    "\n",
    "When you have a dataset with $n$ variables, you are basically have a set of points in $R^{n}$. \n",
    "\n",
    "1.  The first principle direction is the direction of the line that best fits the data as measured by the mean squared error.\n",
    "2.  The second principle direction is the direction of the line that best fits the data while being orthogonal to the first principle direction.\n",
    "\n",
    "This goes on in such way that the $k$-th principal direction is that of the line that best fits the data while being orthogonal to the first, second, and all the way up to $(k-1)$-th principle direction.\n",
    "\n",
    "\n",
    "Let's consider an orthonormal basis $\\{v_{1},\\cdots,v_{n}\\}$ of $R^{n}$ in which $v_{j}$ points in the direction of the $j$-th principal component. The vectors in this orthonormal basis will be of the form $v_{j} = (v_{j}^{1},\\cdots,v_{j}^{n}) \\in R^{n}$.\n",
    "\n",
    "When using principal component analysis, we compute the vector of the aforementioned basis. Then we define variables\n",
    "\n",
    "$$\n",
    "\\overrightarrow{x}_{j} = v_{j}^{1}x_{1} + \\cdots +v_{j}^{n}x_{n}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PCA` class uses `fit` method that analyzes the data and figures out the best way to reduce its dimensionality using principal component analysis. Follow by transform, which can then transform any data in the way it learned to do when `fit` was invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA method with targeted dimension of 8\n",
    "pca = PCA(n_components=8)\n",
    "\n",
    "xs_tr = pca.fit_transform(x_tr)\n",
    "xs_test = pca.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"test-align: center;\">\n",
    "    <img src=\"../../images_QML/QSVM_5_angle_encoding.png\" \n",
    "         alt=\"QSVM_5_angle_encoding\" \n",
    "         style=\"width: 600px; height: 200px;\">\n",
    "         <p style = \"front-size: 16px; front-style: italic; color: gray; \n",
    "         margin-top: 5px;\">\n",
    "            Figure. Angle encoding of an input using different rotation angle.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqubits = 8\n",
    "dev = qml.device(name = 'default.qubit', wires = nqubits) \n",
    "\n",
    "# Apply Angle encoding\n",
    "@qml.qnode(dev)\n",
    "def kernel_circ(a,b):\n",
    "    qml.AngleEmbedding(a, wires = range(nqubits))\n",
    "    qml.adjoint(qml.AngleEmbedding(b, wires=range(nqubits)))\n",
    "    return qml.probs(wires=range(nqubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle encoding test score: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "svm = SVC(kernel=qkernal).fit(xs_tr, y_tr) \n",
    "\n",
    "score_test_angle = accuracy_score(svm.predict(xs_test), y_test)\n",
    "print(f\"Angle encoding test score: {score_test_angle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the result of around 92.3% of accuracy and should have less time then the amplitude encoding with all 13 varaibels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"test-align: center;\">\n",
    "    <img src=\"../../images_QML/QSVM_6_ZZ_feature.png\" \n",
    "         alt=\"QSVM_6_ZZ_feature\" \n",
    "         style=\"width: 600px; height: 200px;\">\n",
    "         <p style = \"front-size: 16px; front-style: italic; color: gray; \n",
    "         margin-top: 5px;\">\n",
    "            Figure. ZZ feature map of three qubits with inputs x\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a QSVM on the reduced dataset using our own implementation of the ZZ feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# creating a ZZFeatureMap\n",
    "def ZZFeatureMap(nqubits, data):\n",
    "    # Number of variables that we will load:\n",
    "    # Could be smaller thatn the number of qubits\n",
    "    nload = min(len(data), nqubits)\n",
    "    \n",
    "    # apply Hadamard and Rz rotation for each qubit,\n",
    "    for i in range(nload):\n",
    "        qml.Hadamard(i)\n",
    "        qml.RZ(2.0 * data[i], wires=i)\n",
    "        \n",
    "    for pair in list(combinations(range(nload),2)):\n",
    "        q0 = pair[0]\n",
    "        q1 = pair[1]\n",
    "\n",
    "        qml.CZ(wires=[q0, q1])\n",
    "        qml.RZ(2.0 * (np.pi - data[q0]) * (np.pi - data[q1]), wires = q1)\n",
    "        qml.CZ(wires=[q0, q1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `combinations` function from the `itertools` module. This function take 2 arguments: an array `arr` and an integer `l`. And it returns an array with all the sorted tuples of length `l` with elements from the array `arr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use it as our kernel function and train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nqubits = 4\n",
    "dev = qml.device(name = 'default.qubit', wires = nqubits) \n",
    "\n",
    "@qml.qnode(dev)\n",
    "def kernel_circ(a,b):\n",
    "    ZZFeatureMap(nqubits, a)\n",
    "    qml.adjoint(ZZFeatureMap)(nqubits, b) ## qml.adjoint(fn: Operator)(nqubits, b)\n",
    "    return qml.probs(wires=range(nqubits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZZ feature map encoding test score: 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "svm = SVC(kernel=qkernal).fit(xs_tr, y_tr) \n",
    "score_test_ZZ = accuracy_score(svm.predict(xs_test), y_test)\n",
    "print(f\"ZZ feature map encoding test score: {score_test_ZZ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a fact that `qml.adjoint` is acting on the `ZZFeatureMap` function itself, not on its qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Feature map                 | Test Score |\n",
      "------------------------------------------\n",
      "Amplitude encoding          |      0.923 | \n",
      "Angle encoding              |      0.923 |\n",
      "ZZ feature map              |      0.846 | \n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"------------------------------------------\")\n",
    "print(f\"Feature map                 | Test Score |\")\n",
    "print(f\"------------------------------------------\")\n",
    "print(f\"Amplitude encoding          | {score_test_amp:10.3f} | \")\n",
    "print(f\"Angle encoding              | {score_test_angle:10.3f} |\")\n",
    "print(f\"ZZ feature map              | {score_test_ZZ:10.3f} | \")\n",
    "print(f\"------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Version Information ==========\n",
      "Python              : 3.11.11 (main, Dec 11 2024, 10:28:39) [Clang 14.0.6 ]\n",
      "Operating System    : Darwin 24.3.0 (64bit)\n",
      "=========================================\n",
      "Pennylane           : 0.26.0\n",
      "pennylane_lightning : 0.28.0 \n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import pennylane\n",
    "import pennylane_lightning\n",
    "\n",
    "\n",
    "print(\"=\"*10 + \" Version Information \" + \"=\"*10)\n",
    "print(f\"Python              : {sys.version}\")\n",
    "print(f\"Operating System    : {platform.system()} {platform.release()} ({platform.architecture()[0]})\")\n",
    "print(\"=\"*41)\n",
    "print(f\"Pennylane           : {pennylane.__version__}\")\n",
    "print(f\"pennylane_lightning : {pennylane_lightning.__version__} \")\n",
    "print(\"=\"*41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]. Combarro, E. F., & Gonz√°lez-Castillo, S. (2023). A practical guide to quantum machine learning and quantum optimisation: Hands-on approach to modern quantum algorithms. Packt Publishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
